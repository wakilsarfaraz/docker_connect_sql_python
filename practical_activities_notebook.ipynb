{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 1: Connecting to the Sakila Database and Running a Simple SQL Query\n",
    "\n",
    "## Objective\n",
    "In this activity, you will establish a connection to the Sakila database using the `pyodbc` library and execute a simple SQL query to retrieve customer information. By the end of this exercise, you should be able to display the first and last names of customers from the database.\n",
    "\n",
    "---\n",
    "\n",
    "### Requirements\n",
    "Before starting this activity, ensure the following:  \n",
    "1. The Sakila database is created and set up on A Cloud Guru (ACG) Azure Sandbox.  \n",
    "2. You have access to the connection strings required to connect to the database.   \n",
    "   - Follow the steps outlined in the `autoGeneratedNotebookForPipeline.ipynb` under the subheadings:  \n",
    "     - **\"Preparing the SQL Database\"**  \n",
    "     - **\"Steps to Acquire the Connection Strings\"**  \n",
    "\n",
    "If the Sakila database is not yet created or the connection strings are not available, complete these steps first before proceeding.  \n",
    "\n",
    "---\n",
    "\n",
    "### Instructions\n",
    "1. Fill in the blanks to complete the Python code below.  \n",
    "2. Execute the code to verify the connection to the Sakila database.  \n",
    "3. Run a simple query to retrieve customer data and print the results to the terminal.  \n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Import Required Libraries\n",
    "```python\n",
    "import pyodbc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:\n",
    "Fill in the placeholders with the appropriate connection details for the Sakila database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = \"<YOUR_SERVER>\" # This is unique to the instance you are running. Starts with tcp: and ends with .net\n",
    "database = \"sakila\" # For this and the activities to follow, this stays the same.\n",
    "username = \"<YOUR_USERNAME>\" # Replace this by \"corndeladmin\"\n",
    "password = \"<YOUR_PASSWORD>\" # Replace this by \"Password01\"\n",
    "\n",
    "connection_string = (\n",
    "    \"Driver={ODBC Driver 18 for SQL Server};\"\n",
    "    \"Server=\" + server + \";\"\n",
    "    \"Database=\" + database + \";\"\n",
    "    \"Uid=\" + username + \";\"\n",
    "    \"Pwd=\" + password + \";\"\n",
    "    \"Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\")\n",
    "\n",
    "# Establish the connection\n",
    "connection = pyodbc.connect(connection_string)\n",
    "cursor = connection.cursor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: \n",
    "Edit the following code to execute the query for retrieving the first and last name of customers from the customer table in the Sakila Database. The query we want to execute is `SELECT TOP 5 first_name, last_name FROM customer;`. Assign this query to the variable `query` as a string as shown in the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARY SMITH\n",
      "PATRICIA JOHNSON\n",
      "LINDA WILLIAMS\n",
      "BARBARA JONES\n",
      "ELIZABETH BROWN\n"
     ]
    }
   ],
   "source": [
    "query = \"<YOUR SQL QUERY>\" # Replace this by the query shown in the instructions.\n",
    "cursor.execute(query) # This line executes the query inside the database.\n",
    "rows = cursor.fetchall() # This line fetches the output of the query.\n",
    "for row in rows: # This for loop prints the output of the query line by line on the terminal or consol.\n",
    "    print(row.first_name, row.last_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "Ensure the connection to the database is properly closed after the query execution. Closing the database connection after running a query is important because it frees up resources and prevents the system from slowing down. If connections stay open, they can block others from accessing the database, cause errors, and even make the database crash if too many connections pile up. It also helps keep the data safe and avoids issues like accidental changes or data being locked. Simply put, closing the connection is like locking a door when you leave a room, it keeps things secure, organised, and ready for the next person to use. Run the following cell to close the connection we have just established."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1 Follow-up Exercise: Querying the Customer Table\n",
    "\n",
    "In this exercise, you will continue using the same code from Activity 1 to run additional queries and gather insights from the customer table. \n",
    "\n",
    "**Instructions:**\n",
    "1. Run each query one by one and review the results.\n",
    "2. Answer the questions provided after each query.\n",
    "3. Reflect on the structure of the results and the data types.\n",
    "\n",
    "---\n",
    "\n",
    "**QUESTION 1**  \n",
    "How many records does the customer table have? \n",
    "\n",
    "```sql\n",
    "SELECT COUNT(*) FROM customer;\n",
    "```\n",
    "**QUESTION 2**  \n",
    "Use the SELECT and COUNT to find how many records are there in the payment table?\n",
    "\n",
    "```sql\n",
    "SELECT COUNT(*) FROM payment;\n",
    "```\n",
    "\n",
    "**QUESTION 3**  \n",
    "Extract the columns customer_id, amount, and payment_date from the payment table.\n",
    "\n",
    "```sql\n",
    "SELECT TOP 10 customer_id, amount, payment_date FROM payment;\n",
    "```\n",
    " \n",
    "### Important Note on Using LIMIT with Sakila Database in Azure Sandbox\n",
    "\n",
    "In the Sakila database hosted in the Azure SQL learning environment, the `LIMIT` command will **not** work as it is typically used in databases like PostgreSQL or MySQL.  \n",
    "\n",
    "Instead, to retrieve a specific number of records, use the **`SELECT TOP N`** command. This is the standard way to limit rows in Microsoft SQL Server, which is the database engine running in Azure.  \n",
    "\n",
    "**Example:**  \n",
    "To return the first 10 records from the `customer` table, use the following:  \n",
    "```sql\n",
    "SELECT TOP 10 * FROM customer;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 2: Saving and Running SQL Files from Python\n",
    "\n",
    "## Objective\n",
    "In this activity, you will save an SQL query to a `.sql` file and run it from Python using the `pyodbc` library. This will help you understand how to manage and automate SQL queries stored as files.\n",
    "\n",
    "---\n",
    "\n",
    "### Requirements\n",
    "Before starting this activity, ensure the following:  \n",
    "1. You have completed **Activity 1** and successfully connected to the Sakila database.  \n",
    "2. The Sakila database is created and connection strings are ready for use.  \n",
    "3. A folder named `sqlFiles` exists in your project directory.  \n",
    "\n",
    "---\n",
    "\n",
    "### Instructions\n",
    "Follow the steps below to save and execute an SQL query from a file.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Create the SQL Query\n",
    "Copy the following SQL query and save it as **`query1.sql`** inside the `sqlFiles` folder.  \n",
    "\n",
    "```sql\n",
    "SELECT film_id, title, rental_rate, release_year\n",
    "FROM film\n",
    "WHERE rental_rate > 2.99\n",
    "ORDER BY rental_rate DESC;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions:\n",
    "\n",
    "- Open a text editor or Jupyter Notebook.\n",
    "- Create a new file.\n",
    "- Paste the SQL code above.\n",
    "- Save the file as query1.sql inside the folder called sqlFiles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Write and Edit Python Code to Execute the SQL File\n",
    "Now, edit the following Python code to read the contents of query1.sql, run it, and print the results. Fill in the blanks where necessary with appropriate details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 ATTACKS HATE 4.99 2006\n",
      "32 APOCALYPSE FLAMINGOS 4.99 2006\n",
      "31 APACHE DIVINE 4.99 2006\n",
      "28 ANTHEM LUKE 4.99 2006\n",
      "21 AMERICAN CIRCUS 4.99 2006\n",
      "20 AMELIE HELLFIGHTERS 4.99 2006\n",
      "13 ALI FOREVER 4.99 2006\n",
      "10 ALADDIN CALENDAR 4.99 2006\n",
      "8 AIRPORT POLLOCK 4.99 2006\n",
      "7 AIRPLANE SIERRA 4.99 2006\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import os\n",
    "\n",
    "# Connection details (reuse from Activity 1 or fill manually)\n",
    "server = \"tcp:<YOUR_SERVER>.database.windows.net\"\n",
    "database = \"sakila\"\n",
    "username = \"<YOUR_USERNAME>\"\n",
    "password = \"<YOUR_PASSWORD>\"\n",
    "\n",
    "connection_string = (\n",
    "    \"Driver={ODBC Driver 18 for SQL Server};\"\n",
    "    \"Server=\" + server + \";\"\n",
    "    \"Database=\" + database + \";\"\n",
    "    \"Uid=\" + username + \";\"\n",
    "    \"Pwd=\" + password + \";\"\n",
    "    \"Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\"\n",
    ")\n",
    "\n",
    "# Establish connection\n",
    "connection = pyodbc.connect(connection_string)\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Path to SQL file\n",
    "sql_file_path = os.path.join('sqlFiles', 'query1.sql')\n",
    "\n",
    "# Read and execute SQL query\n",
    "with open(sql_file_path, 'r') as file:\n",
    "    sql_query = file.read()\n",
    "    cursor.execute(sql_query)\n",
    "\n",
    "    # Fetch and print results\n",
    "    rows = cursor.fetchall()\n",
    "    for row in rows:\n",
    "        print(row.film_id, row.title, row.rental_rate, row.release_year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Close the Connection\n",
    "Always remember to close the connection after executing a query. Run the following cell to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2 Follow-up Exercise: Execute SQL Queries by running the SQL files\n",
    "\n",
    "In this exercise, you will continue using the same code from Activity 2 to run additional queries. The quetions here are the same as the follow up exercises for Activity 1, however, here you are expected to save each query in an SQL file and execute them using the code in Activity 2. To check if your answers are right or wrong, you will be able to execute these queries on the Query Editor on ACG and compare the answers. \n",
    "\n",
    "**Instructions:**\n",
    "1. Run each query one by one and review the results.\n",
    "2. Answer the questions provided after each query.\n",
    "3. Reflect on the structure of the results and the data types.\n",
    "\n",
    "---\n",
    "\n",
    "**QUESTION 1**  \n",
    "How many records does the customer table have? \n",
    "\n",
    "```sql\n",
    "SELECT COUNT(*) FROM customer;\n",
    "```\n",
    "**QUESTION 2**  \n",
    "Use the SELECT and COUNT to find how many records are there in the payment table?\n",
    "\n",
    "```sql\n",
    "SELECT COUNT(*) FROM payment;\n",
    "```\n",
    "\n",
    "**QUESTION 3**  \n",
    "Extract the columns customer_id, amount, and payment_date from the payment table.\n",
    "\n",
    "```sql\n",
    "SELECT TOP 10 customer_id, amount, payment_date FROM payment;\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 3: Writing Query Results to a File\n",
    "\n",
    "## Objective\n",
    "In this activity, you will run an SQL query from a file and write the output to a text file in a folder named `reports`. This helps automate saving query results for later analysis or sharing.\n",
    "\n",
    "---\n",
    "\n",
    "### Requirements\n",
    "Before starting this activity, ensure the following:  \n",
    "1. You have completed **Activity 2** and successfully executed the SQL query from `query1.sql`.  \n",
    "2. A folder named `reports` exists in your project directory (or you will create it in this activity).  \n",
    "\n",
    "---\n",
    "\n",
    "### Instructions\n",
    "Follow these steps to save query results into a report file.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Modified Python Code to Write Output  \n",
    "\n",
    "Update the following code with the connection details and run it to see that a `.txt` file containing the results of the `query1.sql` is now written inside the `reports` folder.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to reports/query1_results.txt\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import os\n",
    "\n",
    "# Connection details (reuse from previous activities)\n",
    "server = \"tcp:<YOUR_SERVER>.database.windows.net\"\n",
    "database = \"sakila\"\n",
    "username = \"<YOUR_USERNAME>\"\n",
    "password = \"<YOUR_PASSWORD>\"\n",
    "\n",
    "connection_string = (\n",
    "    \"Driver={ODBC Driver 18 for SQL Server};\"\n",
    "    \"Server=\" + server + \";\"\n",
    "    \"Database=\" + database + \";\"\n",
    "    \"Uid=\" + username + \";\"\n",
    "    \"Pwd=\" + password + \";\"\n",
    "    \"Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\"\n",
    ")\n",
    "\n",
    "# Establish connection\n",
    "connection = pyodbc.connect(connection_string)\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Path to SQL file\n",
    "sql_file_path = os.path.join('sqlFiles', 'query1.sql')\n",
    "\n",
    "# Read and execute SQL query\n",
    "with open(sql_file_path, 'r') as file:\n",
    "    sql_query = file.read()\n",
    "    cursor.execute(sql_query)\n",
    "\n",
    "    # Fetch results\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Create reports directory if it doesn't exist\n",
    "    os.makedirs(\"reports\", exist_ok=True)\n",
    "\n",
    "    # Write results to a text file\n",
    "    report_file_path = os.path.join(\"reports\", \"query1_results.txt\")\n",
    "    with open(report_file_path, 'w') as report_file:\n",
    "        report_file.write(\"Film ID | Title | Rental Rate | Release Year\\n\")\n",
    "        report_file.write(\"-\" * 50 + \"\\n\")\n",
    "        for row in rows:\n",
    "            report_file.write(f\"{row.film_id} | {row.title} | {row.rental_rate} | {row.release_year}\\n\")\n",
    "\n",
    "    print(f\"Results saved to {report_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Close the Connection\n",
    "Same as in previous activities, we ensure to close the connection by executing the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 4: Extending the ETL Pipeline – Embedding Film Sales Query  \n",
    "\n",
    "## Objective  \n",
    "In this activity, you will extend the functionality of the existing ETL pipeline (`run_pipeline_as_script.py`) by embedding a new SQL query that retrieves film sales data and ranks the films based on total sales. The goal is to automate the execution of this query and store the results in both the database and a `.txt` report file.  \n",
    "\n",
    "---\n",
    "\n",
    "## Task Description  \n",
    "You have been provided with this repository and the following SQL query. Your task is to embed this query into the ETL pipeline and ensure it runs each time `run_pipeline_as_script.py` is executed.  \n",
    "\n",
    "To maintain consistency with the pipeline's existing structure:  \n",
    "1. Create the necessary SQL files to:  \n",
    "   - **Clear the table** (if it exists).  \n",
    "   - **Create a new table** to store the query results.  \n",
    "   - **Store the provided SQL query**.  \n",
    "2. Place these SQL files in the appropriate subfolders within the `sqlFiles` directory.  \n",
    "3. Write a new function within the pipeline to execute this query and automate the following:  \n",
    "   - Insert the query results into the database.  \n",
    "   - Write the results to a `.txt` file in the `reports` folder.  \n",
    "\n",
    "---\n",
    "\n",
    "## SQL Query – Film Sales with Ranking  \n",
    "\n",
    "```SQL\n",
    "WITH film_sales AS (\n",
    "    SELECT \n",
    "        f.film_id, \n",
    "        SUM(p.amount) AS sales\n",
    "    FROM \n",
    "        film f\n",
    "    INNER JOIN inventory i ON f.film_id = i.film_id\n",
    "    INNER JOIN rental r ON i.inventory_id = r.inventory_id\n",
    "    INNER JOIN payment p ON r.rental_id = p.rental_id\n",
    "    GROUP BY \n",
    "        f.film_id\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    a.film_id, \n",
    "    a.sales, \n",
    "    COUNT(b.film_id) + 1 AS rk\n",
    "FROM \n",
    "    film_sales a\n",
    "LEFT JOIN \n",
    "    film_sales b ON a.sales < b.sales\n",
    "GROUP BY \n",
    "    a.film_id, \n",
    "    a.sales\n",
    "ORDER BY \n",
    "    rk;\n",
    "```\n",
    "#### Instructions and Guide\n",
    "\n",
    "* Run the working pipeline: \n",
    "First, execute `run_pipeline_as_script.py` without making any changes to understand its existing functionalities. Observe how it handles table creation, deletion, and data writing.\n",
    "Pay attention to how SQL files are executed and where output reports are saved.\n",
    "* Use Version Control to Track Changes\n",
    "Implement version control (e.g., Git) to save progress frequently. Commit the current working state of the pipeline before introducing changes to avoid losing functional code. Push to a remote repository or create local checkpoints each time the code executes successfully. This allows you to revert to a stable version if new edits introduce errors.\n",
    "* Break the Task into Small Steps\n",
    "For example:\n",
    "* Step 1: Create the SQL files (`clear_table.sql`, `create_table.sql`, and `film_sales_query.sql`).\n",
    "* Step 2: Write a function in Python to read and execute the SQL query.\n",
    "* Step 3: Automate inserting the results into the database and saving them to a report file.\n",
    "* Test the pipeline after each step and commit changes if it works as expected.\n",
    "* Make effective use of logging and debugging methods as already implemented for the existing functionalities with `run_pipeline_as_script.py`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 5: Embedding the Film Sales Query into a Modular ETL Pipeline\n",
    "\n",
    "## Objective  \n",
    "In this activity, you will embed the film sales SQL query from **Activity 4** into the modular ETL pipeline (`run_pipeline_as_package.py`). This script imports and runs code from the `etl_pipeline` package, following best practices for modular programming.  \n",
    "\n",
    "The goal is to extend the package by creating a new module that executes the film sales query and integrates it into the main ETL workflow.  \n",
    "\n",
    "---\n",
    "\n",
    "## Task Description  \n",
    "The `run_pipeline_as_package.py` script uses a structured `etl_pipeline` package to handle various ETL tasks. This package contains modular code for managing tables, running queries, and writing results.  \n",
    "\n",
    "Your task is to:  \n",
    "1. **Create a new module** inside the `etl_pipeline` package to handle the film sales query.  \n",
    "2. **Reuse** the SQL files (`clear_table.sql`, `create_table.sql`, `film_sales_query.sql`) created in Activity 4.  \n",
    "3. Modify `run_pipeline_as_package.py` to import and run the new module as part of the existing ETL pipeline.  \n",
    "4. Automate the execution of the query and ensure results are inserted into the database and saved as a `.txt` file in the `reports` folder.  \n",
    "---\n",
    "\n",
    "\n",
    "# Activity 6: Enhancing the Auto-Generated Notebook  \n",
    "\n",
    "Inside the `etl_pipeline` folder, there is a module called **`manage_notebook.py`**. This module is responsible for automatically generating a Jupyter Notebook that outlines the entire ETL pipeline, including descriptions and step-by-step instructions for running the pipeline sequentially from start to finish.   \n",
    "\n",
    "Each time **`run_pipeline_as_script.py`** is executed, a fresh notebook named **`autoGeneratedNotebook.ipynb`** is created, reflecting the current state of the pipeline.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Your Task**  \n",
    "Modify **`manage_notebook.py`** to ensure that the next time **`run_pipeline_as_script.py`** is run, the newly generated notebook includes:  \n",
    "\n",
    "1. **A Description** – Clearly explain the purpose and functionality of the new module you created in **Activity 4** (the film sales query module).  \n",
    "2. **Python Code Cells** – Embed the Python function from your new module into the notebook as executable code, ensuring it appears in the correct sequence within the pipeline workflow.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Key Considerations**  \n",
    "- Ensure the description clearly explains the role of the new module in the ETL process (e.g., executing the film sales query and writing results).  \n",
    "- Place the Python code cell after table management and before the output writing section to ensure the pipeline's logical flow is maintained.  \n",
    "- Use existing functions in `manage_notebook.py` to append cells to the notebook dynamically. Follow the patterns used for other sections to maintain consistency.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Reflection**  \n",
    "- How does automating documentation through Jupyter Notebooks improve the maintainability of the pipeline?  \n",
    "- Why is it important to keep the notebook up-to-date with every change in the ETL process?  \n",
    "\n",
    "This activity challenges you to think beyond SQL and Python by automating documentation, making your pipeline more transparent and user-friendly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
