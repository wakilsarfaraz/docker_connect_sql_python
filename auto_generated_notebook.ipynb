{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec3fc34c",
   "metadata": {},
   "source": [
    " # Why Connecting to SQL Database Using Python\n",
    "Being able to connect to and interact with SQL databases is a key part \n",
    "of data engineering. Understanding the content of this notebook is an integral \n",
    "part of successful completion of the module \"Connecting to SQL Databases Using \n",
    "Python\". The module focuses on giving learners practical skills to automate and \n",
    "manage data workflows. Imagine handling large amounts of data stored in \n",
    "SQL databases. Manually querying and extracting this data can be slow \n",
    "and prone to mistakes. As a data engineer, automating these tasks saves \n",
    "time and reduces errors. Python makes this easier with libraries like \n",
    "`pyodbc`, `pandas`, and `sqlalchemy`, helping you connect to databases, \n",
    "run queries, and handle data efficiently. Think of a scenario where the \n",
    "sales team needs regular reports from a company’s SQL database. By using \n",
    "Python to automate data extraction and reporting, you create a process \n",
    "that’s quicker and more reliable. Python also lets you adjust the process \n",
    "as needed, making it easier to adapt to new requirements. This module \n",
    "uses a Jupyter Notebook with five activities, starting with the basics \n",
    "of connecting to databases and progressing to more advanced tasks. The \n",
    "final stretch activity involves adding new features to an existing Python \n",
    "pipeline that connects to an SQL database on ACG. Using `pyodbc`, the pipeline \n",
    "extracts data, runs SQL queries from `.SQL` files, and writes results back to \n",
    "the database and as `.txt` files. By completing this module, learners build \n",
    "skills that reflect real-world tasks. These abilities are valuable for \n",
    "data engineering, data analytics, and data science roles, where Python is \n",
    "used for data handling and automation. The goal of this module is to help \n",
    "learners develop the skills they need to connect to SQL databases, automate \n",
    "workflows, and work more efficiently in data-driven environments.\n",
    "\n",
    "# Instructions for Using this Notebook\n",
    "\n",
    "This notebook is created to help you explore and understand the \n",
    "code used in the automated pipeline found in the script `run_etl_as_script.py`.\n",
    "\n",
    "This notebook is a great starting point for you to experiment with while completing \n",
    "Activities 4, 5, and 6. If at any point you want to reset the notebook to its \n",
    "original working form, you can do so easily. To reset the notebook, simply run the script\n",
    "`run_etl_as_package.py`. This will regenerate the \n",
    "notebook in its fully functional state.\n",
    "\n",
    "The notebook is created automatically by a Python script called `manage_notebook.py`, \n",
    "which can be found inside the `etl_pipeline` folder.\n",
    "\n",
    "The notebook has two main purposes:\n",
    "1. To help you understand the code by running each cell. Each function is \n",
    "explained briefly before the code, allowing you to see how the parts work \n",
    "step by step.\n",
    "2. To support you in Activities 4, 5, and 6 in the notebook \n",
    "`Practical_Activities_Notebook.ipynb`. This notebook is a useful \n",
    "starting point where you can try out and edit the code without \n",
    "worrying about breaking the pipeline. \n",
    "\n",
    "In Activity 6, you will be challenged to modify the script `manage_notebook.py` \n",
    "(which is the script that generates this notebook) to include new\n",
    "functionalities that you will embed into the pipeline to ensure \n",
    "that any new functions you add to the pipeline `run_etl_as_script.py`\n",
    "are included in the automatic generation of this notebook.\n",
    "\n",
    "# Understanding the Tools: SQLAlchemy and pyodbc\n",
    "\n",
    "SQLAlchemy and pyodbc are two tools that help Python connect \n",
    "to SQL databases, but they are not the only ones. There are other \n",
    "tools like psycopg2 for PostgreSQL, MySQL Connector for MySQL, and \n",
    "sqlite3 for SQLite. However, SQLAlchemy and pyodbc are widely used \n",
    "and versatile for different types of databases, making them useful \n",
    "to learn. In this module, we will focus on pyodbc because it is simple \n",
    "to use, easy to set up, and works directly with SQL queries. This makes \n",
    "it a practical choice for connecting to databases and building automated \n",
    "pipelines without adding extra complexity.\n",
    "\n",
    "**SQLAlchemy** lets you work with databases by writing Python \n",
    "code instead of SQL. It uses a method called Object Relational Mapping (ORM), \n",
    "which means you can create and manage database tables by working with Python \n",
    "objects and code. This can make managing databases easier and help with more \n",
    "complex projects. However, learning SQLAlchemy can take time, and it might feel\n",
    "more complicated for simple tasks.\n",
    "\n",
    "**pyodbc** is simpler and allows you to connect directly to a database and \n",
    "run SQL queries from Python. It doesn’t add extra layers or tools – you write \n",
    "the SQL yourself and send it to the database. This makes pyodbc easy to use \n",
    "and quick to learn. However, because you have to write the SQL manually, there’s \n",
    "a higher chance of small errors, and the code can get repetitive for bigger projects.\n",
    "\n",
    "In this module, we will use **pyodbc** because it is straightforward, \n",
    "easy to set up, and works well for running SQL queries and building \n",
    "automated pipelines.\n",
    "\n",
    "\n",
    "  \n",
    "# Connecting to SQL Database using `pyodbc` Library\n",
    "This notebook is a step-by-step guide to help you learn how to \n",
    "use Python functions and libraries to connect to an SQL database \n",
    "using the `pyodbc` library. Each section of the notebook includes \n",
    "an explanation of the code, which you can run in order to understand \n",
    "the workflow step by step.\n",
    "## Preparing the SQL Database\n",
    "Before using Python, we need to decide which SQL database to connect to. \n",
    "For this exercise, we will use the A Cloud Guru (ACG) sandbox to create an SQL database. \n",
    "Once the database is set up, we will obtain the necessary credentials to connect to it using Python. \n",
    "A detailed guide on how to create the SQL database in the ACG sandbox is available in a GitHub \n",
    "repository called [DE-sql-learning-environment-Azure](https://github.com/Corndel/DE-sql-learning-environment-Azure). \n",
    "Following the instructions provided in the repository from step **1 Create the ACG Sandbox for SQL learning** to step **4 Create tables in Azure SQL database and insert data**, by the completion of which you will have created an SQL database called **sakila**. We will need the **connection strings** for this database in order to be able to connect to it using Python library `pyodbc`. \n",
    "## Steps to Acquire the Connection Strings\n",
    "- Visit the SQL databases in the Azure portal (you can search for this service using the search \n",
    "box at the top and type SQL).\n",
    "\n",
    "![step-1](images/step1.png)\n",
    "\n",
    "- In the list of databases that appears, click on the single database \n",
    "**sakila (sakilayaq6nqkx2fcqo/sakila)**.\n",
    "\n",
    "![step-2](images/step2.png)\n",
    "\n",
    "- Click on **Settings** and then on **Connection Strings** in the drop down menu.\n",
    "\n",
    "![step-3](images/step3.png)\n",
    "\n",
    "- Go to the `ODBC`tab to see the connection string.\n",
    "\n",
    "![step-4](images/step4.png)\n",
    "\n",
    "- You will need to copy the underlined parts of the **Connection String** and store \n",
    "it somewhere safely to be used when running the pipeline.\n",
    "\n",
    "![step-5](images/step5.png)\n",
    "\n",
    "- Note that the password *{your_password_here}* is a placeholder within the connection strings, \n",
    "which needs to be replaced with the actual password of the database.\n",
    "\n",
    "- When running the pipeline you will be prompted to enter these three pieces of information:\n",
    "\n",
    "    * Server name (the longest underlined part in the above screenshot, which you will be different \n",
    "    every time you create sakila). \n",
    "    * Username which for this database is **corndeladmin**.\n",
    "    * Passoword, which is **Password01**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1be9fa8",
   "metadata": {},
   "source": [
    "## **Sakila** Database Connection & Creating an ETL Pipeline\n",
    "\n",
    "This guide will walk you through the steps (using the `pyodbc` library) to connect to the SQL \n",
    "database we have just created. You will learn how to extract data from the database, manage \n",
    "tables within it, and write the processed data back into the database. Follow each step carefully \n",
    "to gain a clear understanding of how to build an ETL pipeline when you are using Python to work \n",
    "with SQL databases on virtual machines or on remote platforms online.\n",
    "\n",
    "\n",
    "### The Python Libraries\n",
    "\n",
    "#### 1. `pyodbc`\n",
    "It is a Python library for connecting to SQL databases using ODBC drivers. \n",
    "It allows communication between Python and SQL databases, making it essential \n",
    "for running SQL queries and managing database connections.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. `pandas`\n",
    "This is a Python library for data manipulation and analysis. It provides powerful tools \n",
    "to handle tabular data, such as reading, writing, and processing datasets extracted from \n",
    "the database.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. `os`\n",
    "This is a standard Python library for interacting with the operating system. \n",
    "It enables tasks like file and directory manipulation, crucial for managing input/output \n",
    "files during the ETL process.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. `shutil`\n",
    "It is a standard Python library for high-level file operations. It helps in efficiently \n",
    "clearing or organising folders by removing directories and their contents.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. `logging`\n",
    "This library is used for tracking events during code execution. It provides detailed \n",
    "logs for debugging and monitoring the ETL pipeline, ensuring transparency and easier \n",
    "troubleshooting.\n",
    "\n",
    "#### 5. `getpass`\n",
    "The `getpass` library in Python securely prompts the user for sensitive information, \n",
    "such as passwords, without displaying the input on the screen. It is ideal for creating \n",
    "secure, interactive command-line applications.\n",
    "\n",
    "Let's run the following cell to import all of these libraries. This code after the importation \n",
    "uses the `logging` library to set up logging for the ETL pipeline to track events and errors. \n",
    "It writes logs to a file named `etl_pipeline.log` and displays them on the console. The log \n",
    "messages include the time, log level, and the message for clear and detailed tracking \n",
    "of the pipeline’s progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96c7babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import getpass\n",
    "from etl_pipeline import manage_notebook\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler(\"etl_pipeline.log\"),\n",
    "                        logging.StreamHandler()\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8681a369",
   "metadata": {},
   "source": [
    "#### Function: `clear_folder`\n",
    "\n",
    "This function removes all files and subdirectories within a specified folder. \n",
    "It iterates through each item in the folder, deleting files, symbolic links, and entire \n",
    "directories as needed. The process is logged for transparency, with messages indicating \n",
    "when the operation starts, completes, or encounters an error. This function is particularly \n",
    "useful for ensuring a clean workspace before running an ETL pipeline or similar processes. \n",
    "We will use this function to ensure that our target folder is cleared before the ETL output \n",
    "is written into it. Our target folder for this function is called `reports`, which will be \n",
    "used when we run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3833889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Clears the contents of a specified folder by deleting all files, subdirectories, \n",
    "    and symbolic links within it.\n",
    "\n",
    "    This function recursively deletes all items inside the given folder path, including:\n",
    "    - Regular files\n",
    "    - Symbolic links\n",
    "    - Subdirectories and their contents\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder whose contents need to be cleared.\n",
    "\n",
    "    Logging:\n",
    "        - Logs the start of the clearing process at the INFO level.\n",
    "        - Logs a success message when the folder is cleared at the INFO level.\n",
    "        - Logs any errors encountered during the process at the ERROR level.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If any error occurs while attempting to delete files, subdirectories, \n",
    "        or symbolic links.\n",
    "\n",
    "    Examples:\n",
    "        >>> clear_folder(\"/path/to/directory\")\n",
    "        INFO: Starting to clear the contents of folder: /path/to/directory\n",
    "        INFO: Contents of folder /path/to/directory have been cleared.\n",
    "\n",
    "    Note:\n",
    "        - The function does not delete the folder itself, only its contents.\n",
    "        - Ensure that the folder exists before calling this function. If the folder does not \n",
    "          exist, an error may be raised.\n",
    "        - Use with caution as this action is irreversible.\n",
    "\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting to clear the contents of folder: {folder_path}\")\n",
    "    try:\n",
    "        for item in os.listdir(folder_path):\n",
    "            item_path = os.path.join(folder_path, item)\n",
    "            if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "                os.unlink(item_path)\n",
    "            elif os.path.isdir(item_path):\n",
    "                shutil.rmtree(item_path)\n",
    "        logging.info(f\"Contents of folder {folder_path} have been cleared.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while clearing folder {folder_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91741fce",
   "metadata": {},
   "source": [
    "#### Function: `manage_tables`\n",
    "The `manage_tables` function is responsible for resetting the structure of specific database tables. \n",
    "It connects to the database using the provided connection string and executes SQL scripts to drop \n",
    "existing tables (`payment_summary_table` and `duration_summary_table`) and recreate them. \n",
    "The function reads the SQL commands from files located in the `queries` folder, ensuring the \n",
    "database is prepared for new data. It handles errors, such as database connection issues or \n",
    "missing SQL files, and logs the process for transparency and troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c9c3942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_tables(connection_string):\n",
    "    \"\"\"\n",
    "    Drops and recreates database tables by executing SQL scripts.\n",
    "\n",
    "    This function reads SQL scripts from predefined file paths to drop existing tables \n",
    "    and create new ones. It ensures that the database schema is updated by executing \n",
    "    the scripts sequentially.\n",
    "\n",
    "    Args:\n",
    "        connection_string (str): A valid database connection string used to connect to \n",
    "                                 the database.\n",
    "\n",
    "    Logging:\n",
    "        - Logs the start of the table management process.\n",
    "        - Logs success after recreating tables.\n",
    "        - Logs errors encountered during execution.\n",
    "\n",
    "    SQL File Structure:\n",
    "        - Drop Table SQL Files:\n",
    "            - 'sqlFiles/tableManagement/drop_payment_summary_table.sql'\n",
    "            - 'sqlFiles/tableManagement/drop_duration_summary_table.sql'\n",
    "            - 'sqlFiles/tableManagement/drop_profitable_actors_table.sql'\n",
    "        - Create Table SQL Files:\n",
    "            - 'sqlFiles/tableManagement/create_payment_summary_table.sql'\n",
    "            - 'sqlFiles/tableManagement/create_duration_summary_table.sql'\n",
    "            - 'sqlFiles/tableManagement/create_profitable_actors_table.sql'\n",
    "\n",
    "    Raises:\n",
    "        Exception: If any error occurs during the database operations.\n",
    "\n",
    "    Examples:\n",
    "        >>> manage_tables(\"Driver={ODBC Driver 18 for SQL Server};\"\n",
    "                          \"Server=server_name;\"\n",
    "                          \"Database=database_name;\"\n",
    "                          \"Uid=username;Pwd=password;\")\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"Starting to manage tables in the database.\")\n",
    "    try:\n",
    "        connection = pyodbc.connect(connection_string)\n",
    "        cursor = connection.cursor()\n",
    "        drop_payment_table_file = os.path.join('sql_files/table_management', 'drop_payment_summary_table.sql')\n",
    "        drop_duration_table_file = os.path.join('sql_files/table_management', 'drop_duration_summary_table.sql')\n",
    "        drop_profitable_table_file = os.path.join('sql_files/table_management','drop_profitable_actors_table.sql')\n",
    "        create_payment_table_file = os.path.join('sql_files/table_management', 'create_payment_summary_table.sql')\n",
    "        create_duration_table_file = os.path.join('sql_files/table_management', 'create_duration_summary_table.sql')\n",
    "        create_profitable_actors_table_file = os.path.join('sql_files/table_management','create_profitable_actors_table.sql')\n",
    "        def execute_sql_file(file_path):\n",
    "            with open(file_path, 'r') as file:\n",
    "                sql = file.read()\n",
    "                cursor.execute(sql)\n",
    "        execute_sql_file(drop_payment_table_file)\n",
    "        execute_sql_file(drop_duration_table_file)\n",
    "        execute_sql_file(drop_profitable_table_file)\n",
    "        connection.commit()\n",
    "        execute_sql_file(create_payment_table_file)\n",
    "        execute_sql_file(create_duration_table_file)\n",
    "        execute_sql_file(create_profitable_actors_table_file)\n",
    "        connection.commit()\n",
    "        logging.info(\"Tables:\\n\\n                                         payment_summary_table\\n\"         \n",
    "                     \"                                         duration_summary_table\\n\"\n",
    "                     \"                                         profitable_actors_table \\n\\n\"\n",
    "                     \"                                 have been recreated in the database.\")\n",
    "    except pyodbc.Error as e:\n",
    "        logging.error(f\"Error managing tables: {e}\")\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"SQL file not found: {e}\")\n",
    "    finally:\n",
    "        if 'connection' in locals() and connection:\n",
    "            connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a333abc5",
   "metadata": {},
   "source": [
    "#### Function: `calculate_payments`\n",
    "The `calculate_payments` function reads an SQL query from a specified file, executes it on \n",
    "a database, and retrieves the results as a pandas DataFrame. The function connects to the \n",
    "database using a given connection string and processes the query to calculate a payments summary, \n",
    "including columns such as `Records`, `Minimum`, `Maximum`, `Total`, and `Average`. It logs progress \n",
    "and errors for transparency and closes the database connection after execution. The result is \n",
    "returned as a structured DataFrame for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131ffdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_payments(sql_file_path, connection_string):\n",
    "    \"\"\"\n",
    "    Calculates a summary of payments using a SQL query and returns a pandas DataFrame.\n",
    "\n",
    "    This function executes a SQL query from the specified file, processes the query \n",
    "    results, and generates a summary DataFrame with the following columns:\n",
    "    - Records\n",
    "    - Minimum\n",
    "    - Maximum\n",
    "    - Total\n",
    "    - Average\n",
    "\n",
    "    Args:\n",
    "        sql_file_path (str): Path to the SQL file containing the query.\n",
    "        connection_string (str): Database connection string.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the payments summary.\n",
    "\n",
    "    Raises:\n",
    "        pyodbc.Error: If there is an error while executing the query or connecting to the database.\n",
    "\n",
    "    Examples:\n",
    "        >>> payments_df = calculate_payments(\"queries/payments.sql\", connection_string)\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting to calculate payments summary.\")\n",
    "    try:\n",
    "        connection = pyodbc.connect(connection_string)\n",
    "        cursor = connection.cursor()\n",
    "        with open(sql_file_path, 'r') as file:\n",
    "            sql_query = file.read()\n",
    "        cursor.execute(sql_query)\n",
    "        rows = cursor.fetchall()\n",
    "        payments_summary = pd.DataFrame((tuple(t) for t in rows)) \n",
    "        payments_summary.columns = ['Records', 'Minimum', 'Maximum', 'Total', 'Average']\n",
    "        logging.info(\"Payments summary successfully retrieved.\")\n",
    "    except pyodbc.Error as e:\n",
    "        logging.error(f\"Error executing payments query: {e}\")\n",
    "    finally:\n",
    "        if 'connection' in locals() and connection:\n",
    "            connection.close()\n",
    "    return payments_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51123695",
   "metadata": {},
   "source": [
    "#### Function: `calculate_duration`\n",
    "\n",
    "This function retrieves a summary of film durations from an SQL database by executing a query \n",
    "provided in an external SQL file. It connects to the database using the given connection string, \n",
    "reads the query from the specified file, and runs it. The results are then stored in a pandas \n",
    "DataFrame with columns: `Minimum`, `Maximum`, `Total`, and `Average`. Finally, it logs the process \n",
    "and ensures the database connection is closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc5c93fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_duration(sql_file_path, connection_string):\n",
    "    \"\"\"\n",
    "    Calculates a summary of film durations using a SQL query and returns a pandas DataFrame.\n",
    "\n",
    "    This function executes a SQL query from the specified file, processes the query \n",
    "    results, and generates a summary DataFrame with the following columns:\n",
    "    - Minimum\n",
    "    - Maximum\n",
    "    - Total\n",
    "    - Average\n",
    "\n",
    "    Args:\n",
    "        sql_file_path (str): Path to the SQL file containing the query.\n",
    "        connection_string (str): Database connection string.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the duration summary.\n",
    "\n",
    "    Raises:\n",
    "        pyodbc.Error: If there is an error while executing the query or connecting to the database.\n",
    "\n",
    "    Examples:\n",
    "        >>> duration_df = calculate_duration(\"queries/filmduration.sql\", connection_string)\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting to calculate duration summary.\")\n",
    "    try:\n",
    "        connection = pyodbc.connect(connection_string)\n",
    "        cursor = connection.cursor()\n",
    "        with open(sql_file_path, 'r') as file:\n",
    "            sql_query = file.read()\n",
    "        cursor.execute(sql_query)\n",
    "        rows = cursor.fetchall()\n",
    "        duration_summary = pd.DataFrame((tuple(t) for t in rows)) \n",
    "        duration_summary.columns = ['Minimum', 'Maximum', 'Total', 'Average']\n",
    "        logging.info(\"Duration summary successfully retrieved.\")\n",
    "    except pyodbc.Error as e:\n",
    "        logging.error(f\"Error executing duration query: {e}\")\n",
    "    finally:\n",
    "        if 'connection' in locals() and connection:\n",
    "            connection.close()\n",
    "    return duration_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa45fd40",
   "metadata": {},
   "source": [
    "#### Function: `calculate_profitable_actors`\n",
    "\n",
    "The `calculate_profitable_actors` function retrieves a list of the most profitable actors from \n",
    "a database based on an SQL query. It connects to the database using the provided connection string, \n",
    "reads the SQL query from a file, and executes it to fetch the results. These results are converted \n",
    "into a pandas DataFrame with clear column names: `ActorID`, `FirstName`, `LastName`, and \n",
    "`TotalSale`. The function includes error handling to log any issues and ensures the database \n",
    "connection is closed afterwards to manage resources efficiently. This function is a great way \n",
    "for learners to experiment with combining Python, SQL, and pandas to process and analyse data \n",
    "effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83e76413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_profitable_actors(sql_file_path, connection_string):\n",
    "    \"\"\"\n",
    "    Retrieves a summary of the most profitable actors using a SQL query and returns a pandas DataFrame.\n",
    "\n",
    "    This function executes a SQL query from the specified file, processes the query \n",
    "    results, and generates a summary DataFrame with the following columns:\n",
    "    - ActorID\n",
    "    - FirstName\n",
    "    - LastName\n",
    "    - TotalSale\n",
    "\n",
    "    Args:\n",
    "        sql_file_path (str): Path to the SQL file containing the query.\n",
    "        connection_string (str): Database connection string.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the summary of profitable actors.\n",
    "\n",
    "    Raises:\n",
    "        pyodbc.Error: If there is an error while executing the query or connecting to the database.\n",
    "\n",
    "    Examples:\n",
    "        >>> profitable_actors_df = calculate_profitable_actors(\"queries/profitable_actors.sql\", connection_string)\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting to calculate profitable actors.\")\n",
    "    try:\n",
    "        connection = pyodbc.connect(connection_string)\n",
    "        cursor = connection.cursor()\n",
    "        with open(sql_file_path, 'r') as file:\n",
    "            sql_query = file.read()\n",
    "        cursor.execute(sql_query)\n",
    "        rows = cursor.fetchall()\n",
    "        profitable_actors = pd.DataFrame((tuple(t) for t in rows)) \n",
    "        profitable_actors.columns = ['ActorID', 'FirstName', 'LastName', 'TotalSale']\n",
    "        logging.info(\"Profitable actors successfully retrieved.\")\n",
    "    except pyodbc.Error as e:\n",
    "        logging.error(f\"Error executing duration query: {e}\")\n",
    "    finally:\n",
    "        if 'connection' in locals() and connection:\n",
    "            connection.close()\n",
    "    return profitable_actors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f8fe8",
   "metadata": {},
   "source": [
    "#### Function: `write_dataframe_to_db`\n",
    "\n",
    "The `write_dataframe_to_db` function inserts the rows of a pandas DataFrame into a specified \n",
    "SQL database table. It connects to the database using the provided connection string, then \n",
    "iterates through each row of the DataFrame and executes an SQL `INSERT` statement to write the data. \n",
    "The function dynamically matches column names and values using placeholders to ensure compatibility. \n",
    "It also handles errors gracefully by logging issues and ensuring the database connection is properly \n",
    "closed after the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b4e5b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframe_to_db(dataframe, table_name, connection_string):\n",
    "    \"\"\"\n",
    "    Inserts rows from a pandas DataFrame into a specified database table.\n",
    "\n",
    "    This function takes a pandas DataFrame, converts its rows into SQL INSERT statements, \n",
    "    and writes them to the specified table in the database.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): The DataFrame containing the data to insert.\n",
    "        table_name (str): The name of the database table to insert data into.\n",
    "        connection_string (str): A valid database connection string used to connect to \n",
    "                                 the database.\n",
    "\n",
    "    Logging:\n",
    "        - Logs the start of the data insertion process.\n",
    "        - Logs success after the data is written.\n",
    "        - Logs errors encountered during execution.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If any error occurs while inserting data into the database.\n",
    "\n",
    "    Examples:\n",
    "        >>> import pandas as pd\n",
    "        >>> data = {'column1': [1, 2], 'column2': ['A', 'B']}\n",
    "        >>> df = pd.DataFrame(data)\n",
    "        >>> write_dataframe_to_db(df, \"my_table\", \n",
    "                                  \"Driver={ODBC Driver 18 for SQL Server};\"\n",
    "                                  \"Server=server_name;\"\n",
    "                                  \"Database=database_name;\"\n",
    "                                  \"Uid=username;Pwd=password;\")\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting to write DataFrame to table: {table_name}\")\n",
    "    try:\n",
    "        connection = pyodbc.connect(connection_string)\n",
    "        cursor = connection.cursor()\n",
    "        # Insert rows into the database\n",
    "        for index, row in dataframe.iterrows():\n",
    "            placeholders = ', '.join(['?'] * len(row))\n",
    "            columns = ', '.join(dataframe.columns)\n",
    "            sql = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "            cursor.execute(sql, tuple(row))\n",
    "        connection.commit()\n",
    "        logging.info(f\"Data successfully written to table: {table_name}.\")\n",
    "    except pyodbc.Error as e:\n",
    "        logging.error(f\"Error writing to database table {table_name}: {e}\")\n",
    "    finally:\n",
    "        if 'connection' in locals() and connection:\n",
    "            connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08247d73",
   "metadata": {},
   "source": [
    "#### Function: `write_local_txt_output`\n",
    "\n",
    "This function saves the contents of a pandas DataFrame as a tab-separated text file in a specified \n",
    "folder. It first ensures the folder exists (creating it if necessary), then writes the DataFrame to \n",
    "a file with the given name. The file does not include row indices, making it cleaner for sharing or \n",
    "further processing. If successful, the function logs the file’s location and returns its path. \n",
    "In case of an error, it logs the issue and returns `None`, ensuring clear feedback for \n",
    "troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "492d16ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_local_txt_output(dataframe, folder_path, file_name):\n",
    "    \"\"\"\n",
    "    Writes a pandas DataFrame to a local text file in tab-delimited format.\n",
    "\n",
    "    This function ensures the target folder exists (creates it if necessary), then writes \n",
    "    the given DataFrame to a text file with tab-separated values. The resulting file is saved \n",
    "    in the specified folder with the provided file name.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): The DataFrame to write to the text file.\n",
    "        folder_path (str): The path to the folder where the file will be saved. If the folder \n",
    "                           does not exist, it will be created.\n",
    "        file_name (str): The name of the text file to create.\n",
    "\n",
    "    Returns:\n",
    "        str: The full path to the created file if the operation is successful.\n",
    "        None: If an error occurs during the operation.\n",
    "\n",
    "    Logging:\n",
    "        - Logs the start of the file writing process at the INFO level.\n",
    "        - Logs the successful completion of the operation at the INFO level.\n",
    "        - Logs any errors encountered during the operation at the ERROR level.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Logs any error that occurs during folder creation, file writing, or \n",
    "                   DataFrame processing.\n",
    "\n",
    "    Examples:\n",
    "        >>> import pandas as pd\n",
    "        >>> data = {'Column1': [1, 2], 'Column2': ['A', 'B']}\n",
    "        >>> df = pd.DataFrame(data)\n",
    "        >>> write_local_txt_output(df, \"output_folder\", \"output_file.txt\")\n",
    "        INFO: Starting to write DataFrame to text file: output_file.txt\n",
    "        INFO: Processed data successfully written to output_folder/output_file.txt\n",
    "\n",
    "    Notes:\n",
    "        - The file is written in tab-delimited format (`sep='\\t'`).\n",
    "        - The index is not included in the output file (`index=False`).\n",
    "        - Ensure the DataFrame contains valid data before calling this function.\n",
    "\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting to write DataFrame to text file: {file_name}\")\n",
    "    try:\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        dataframe.to_csv(file_path, sep='\\t', index=False)\n",
    "        logging.info(f\"Processed data successfully written to {file_path}\")\n",
    "        return file_path\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while writing to text file {file_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b6eaaa",
   "metadata": {},
   "source": [
    "#### Running the pipeline\n",
    "\n",
    "This section of the code serves as the entry point for the script. It begins by prompting the user \n",
    "to provide the SQL Server address, username, and password, ensuring secure input for the connection. \n",
    "A connection string is then constructed to establish communication with the `sakila` database using \n",
    "the specified SQL Server. The script prepares a target folder (`reports`) by clearing any existing \n",
    "content and processes the database tables using custom queries.\n",
    "\n",
    "It performs the following tasks:\n",
    "1. Executes the SQL query for payments and calculates summary data.\n",
    "2. Executes the SQL query for film durations and calculates summary data.\n",
    "3. Executes the SQL query for profitable actors and calculates a table of the most profitable actors.\n",
    "4. Saves the results into three summary tables in the database: `payment_summary_table`, `duration_summary_table`, and `profitable_actors_table`.\n",
    "5. Exports also the same results into local `.txt` files, stored in the `reports` folder.\n",
    "\n",
    "The pipeline ensures that all relevant data is processed, stored, and made accessible for further use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33be1391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 09:10:57,307 - INFO - Starting to clear the contents of folder: reports\n",
      "2025-01-10 09:10:57,309 - INFO - Contents of folder reports have been cleared.\n",
      "2025-01-10 09:10:57,310 - INFO - Starting to manage tables in the database.\n",
      "2025-01-10 09:11:02,517 - INFO - Tables:\n",
      "\n",
      "                                         payment_summary_table\n",
      "                                         duration_summary_table\n",
      "                                         profitable_actors_table \n",
      "\n",
      "                                 have been recreated in the database.\n",
      "2025-01-10 09:11:02,518 - INFO - Starting to calculate payments summary.\n",
      "2025-01-10 09:11:05,181 - INFO - Payments summary successfully retrieved.\n",
      "2025-01-10 09:11:05,400 - INFO - Starting to calculate duration summary.\n",
      "2025-01-10 09:11:08,053 - INFO - Duration summary successfully retrieved.\n",
      "2025-01-10 09:11:08,274 - INFO - Starting to calculate profitable actors.\n",
      "2025-01-10 09:11:11,364 - INFO - Profitable actors successfully retrieved.\n",
      "2025-01-10 09:11:11,589 - INFO - Starting to write DataFrame to table: payment_summary_table\n",
      "2025-01-10 09:11:14,446 - INFO - Data successfully written to table: payment_summary_table.\n",
      "2025-01-10 09:11:14,447 - INFO - Starting to write DataFrame to table: duration_summary_table\n",
      "2025-01-10 09:11:17,326 - INFO - Data successfully written to table: duration_summary_table.\n",
      "2025-01-10 09:11:17,327 - INFO - Starting to write DataFrame to table: profitable_actors_table\n",
      "2025-01-10 09:12:04,899 - INFO - Data successfully written to table: profitable_actors_table.\n",
      "2025-01-10 09:12:04,900 - INFO - Starting to write DataFrame to text file: payment_summary.txt\n",
      "2025-01-10 09:12:04,903 - INFO - Processed data successfully written to reports/payment_summary.txt\n",
      "2025-01-10 09:12:04,903 - INFO - Starting to write DataFrame to text file: duration_summary.txt\n",
      "2025-01-10 09:12:04,905 - INFO - Processed data successfully written to reports/duration_summary.txt\n",
      "2025-01-10 09:12:04,905 - INFO - Starting to write DataFrame to text file: profitable_actors.txt\n",
      "2025-01-10 09:12:04,907 - INFO - Processed data successfully written to reports/profitable_actors.txt\n"
     ]
    }
   ],
   "source": [
    "# Main block starts here\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    server = input(\"Please enter the SQL Server address (hint: starts with tcp and ends with .net): \").strip()\n",
    "    username = input(\"Please enter your Username:\").strip()\n",
    "    password = getpass.getpass(\"Please enter your Password: \").strip()\n",
    "    connection_string =   str(\n",
    "    f\"Driver={{ODBC Driver 18 for SQL Server}};\"\n",
    "    f\"Server={server},1433;\"\n",
    "    f\"Database=sakila;\"\n",
    "    f\"Uid={username};\"\n",
    "    f\"Pwd={password};\"\n",
    "    f\"Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;\")\n",
    "    target_folder = \"reports\"\n",
    "    clear_folder(target_folder)\n",
    "    manage_tables(connection_string)\n",
    "    payments_df = calculate_payments(\"sql_files/queries/payments.sql\", connection_string)\n",
    "    duration_df = calculate_duration(\"sql_files/queries/film_duration.sql\", connection_string)\n",
    "    profitable_actors_df = calculate_profitable_actors(\"sql_files/queries/profitable_actors.sql\", connection_string)\n",
    "    write_dataframe_to_db(payments_df, \"payment_summary_table\", connection_string)\n",
    "    write_dataframe_to_db(duration_df, \"duration_summary_table\", connection_string)\n",
    "    write_dataframe_to_db(profitable_actors_df,\"profitable_actors_table\", connection_string)\n",
    "    write_local_txt_output(payments_df, \"reports\", \"payment_summary.txt\")\n",
    "    write_local_txt_output(duration_df, \"reports\", \"duration_summary.txt\")\n",
    "    write_local_txt_output(profitable_actors_df, \"reports\", \"profitable_actors.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
